{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "from Chatbot import Chatbot\n",
    "import os\n",
    "from langchain.document_loaders import DirectoryLoader,PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import openai\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chat_models import ChatOpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf= \"article.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot = Chatbot()\n",
    "\n",
    "loader  = PyPDFDirectoryLoader(\"directory/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Article\\nPhasing out the blast furnace to meet global\\nclimate targets\\nIron and steel production accounts for 7% of anthropogenic greenhouse gas\\nemissions. We estimate expected future emissions from steel based on animproved approach to committed emissions accounting that uses actualequipment-level data. Without starting to phase out unmitigated blast furnaces\\nsoon the operation of current steel production equipment will consume signiﬁcant\\namounts of the remaining 1.5\\n/C14C carbon budget. The age of emission-intensive\\nassets should be better monitored, and research should explore regulatoryphaseouts of such assets.\\nValentin Vogl, Olle Olsson,\\nBjo¨rn Nykvist\\nvalentin.vogl@miljo.lth.se\\nHighlights\\nFirst estimation of committed\\nemissions based on actual\\nindustry equipment data\\nThe median historic blast furnace\\ncampaign length is 17 years\\nCO 2emissions of 21 Gt to be\\nexpected for immediate blastfurnace phase-out case\\n10 years of inaction and steel\\nconsumes 12% of the remaining1.5', metadata={'source': 'directory\\\\article.pdf', 'page': 0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000 ,chunk_overlap=100)\n",
    "chunks = splitter.split_documents(docs)\n",
    "vectorestore = Chroma.from_documents(chunks, embeddings)\n",
    "#chain = chatbot.build_chain(vectorstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = ConversationalRetrievalChain.from_llm(llm = llm, retriever = vectorestore.as_retriever(),\n",
    "                                            return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Chatbot:\n",
    "    def __init__(self):\n",
    "        self.splitter = RecursiveCharacterTextSplitter(chunk_size=1000 ,chunk_overlap=100)\n",
    "        self.embeddings = OpenAIEmbeddings()\n",
    "        self.llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "    def load_document(self, files):\n",
    "        text=\"\"\n",
    "        for file in files:\n",
    "            pdf = PdfReader(file)\n",
    "            for page in pdf.pages:\n",
    "                text += page.extract_text()\n",
    "        return text\n",
    "    \n",
    "    def load_directory(self, directory):\n",
    "        loader = PyPDFDirectoryLoader(directory)\n",
    "        return loader.load()\n",
    "\n",
    "    \n",
    "    def split_text(self, text):\n",
    "        return self.splitter.split_documents(text)\n",
    "\n",
    "\n",
    "    def vectorize(self, chunks):\n",
    "        vectorestore = Chroma.from_documents(chunks, self.embeddings)\n",
    "        return vectorestore\n",
    "    \n",
    "    def build_chain(self, vectorstore):\n",
    "        chain = ConversationalRetrievalChain.from_llm(llm = self.llm, retriever = vectorstore.as_retriever(),\n",
    "                                            return_source_documents=True)\n",
    "        return chain\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument: '\\x07'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Omar\\Desktop\\furnace_whisper\\test.ipynb Cell 4\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Omar/Desktop/furnace_whisper/test.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m chatbot \u001b[39m=\u001b[39m Chatbot()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Omar/Desktop/furnace_whisper/test.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mif\u001b[39;00m pdf:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Omar/Desktop/furnace_whisper/test.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     text \u001b[39m=\u001b[39m chatbot\u001b[39m.\u001b[39;49mload_document(pdf)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Omar/Desktop/furnace_whisper/test.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     chunks \u001b[39m=\u001b[39m chatbot\u001b[39m.\u001b[39msplit_text(text)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Omar/Desktop/furnace_whisper/test.ipynb#W2sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     vectorstore \u001b[39m=\u001b[39m chatbot\u001b[39m.\u001b[39mvectorize(chunks)\n",
      "File \u001b[1;32mc:\\Users\\Omar\\Desktop\\furnace_whisper\\Chatbot.py:23\u001b[0m, in \u001b[0;36mChatbot.load_document\u001b[1;34m(self, files)\u001b[0m\n\u001b[0;32m     21\u001b[0m text\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     22\u001b[0m \u001b[39mfor\u001b[39;00m file \u001b[39min\u001b[39;00m files:\n\u001b[1;32m---> 23\u001b[0m     pdf \u001b[39m=\u001b[39m PdfReader(file)\n\u001b[0;32m     24\u001b[0m     \u001b[39mfor\u001b[39;00m page \u001b[39min\u001b[39;00m pdf\u001b[39m.\u001b[39mpages:\n\u001b[0;32m     25\u001b[0m         text \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m page\u001b[39m.\u001b[39mextract_text()\n",
      "File \u001b[1;32mc:\\Users\\Omar\\.conda\\envs\\gpt\\Lib\\site-packages\\PyPDF2\\_reader.py:317\u001b[0m, in \u001b[0;36mPdfReader.__init__\u001b[1;34m(self, stream, strict, password)\u001b[0m\n\u001b[0;32m    311\u001b[0m     logger_warning(\n\u001b[0;32m    312\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPdfReader stream/file object is not in binary mode. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    313\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIt may not be read correctly.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    314\u001b[0m         \u001b[39m__name__\u001b[39m,\n\u001b[0;32m    315\u001b[0m     )\n\u001b[0;32m    316\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(stream, (\u001b[39mstr\u001b[39m, Path)):\n\u001b[1;32m--> 317\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(stream, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mas\u001b[39;00m fh:\n\u001b[0;32m    318\u001b[0m         stream \u001b[39m=\u001b[39m BytesIO(fh\u001b[39m.\u001b[39mread())\n\u001b[0;32m    319\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mread(stream)\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 22] Invalid argument: '\\x07'"
     ]
    }
   ],
   "source": [
    "chatbot = Chatbot()\n",
    "if pdf:\n",
    "    text = chatbot.load_document(pdf)\n",
    "    chunks = chatbot.split_text(text)\n",
    "    vectorstore = chatbot.vectorize(chunks)\n",
    "    chain = chatbot.build_chain(vectorstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "from Chatbot import Chatbot\n",
    "import os\n",
    "\n",
    "st.title(\"Furnace Whisper\")\n",
    "\n",
    "# load the API key from the .env file\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "with st.sidebar:\n",
    "\n",
    "   # pdfs = st.file_uploader(\"Upload a PDF\", type=[\"pdf\"], accept_multiple_files=True)\n",
    "    chatbot = Chatbot()\n",
    "    \n",
    "    docs = chatbot.load_directory(\"directory/\")\n",
    "    chunks = chatbot.split_text(docs)\n",
    "    vectorstore = chatbot.vectorize(chunks)\n",
    "    chain = chatbot.build_chain(vectorstore)\n",
    "    \n",
    "    st.write(\"Ready to chat!\")\n",
    "    st.write(\"Ask a question below and I'll try to answer it.\")\n",
    "    question = st.text_input(\"Question:\")\n",
    "    if question:\n",
    "        chat_history = []\n",
    "\n",
    "        response = chain({\"question\": question,  \"chat_history\": chat_history})\n",
    "        chat_history.append((question, response[\"answer\"]))\n",
    "        st.write(response[\"answer\"])\n",
    "\"\"\"  \n",
    "    if pdfs:\n",
    "        text = chatbot.load_document(pdfs)\n",
    "        chunks = chatbot.split_text(text)\n",
    "        vectorstore = chatbot.vectorize(chunks)\n",
    "        chain = chatbot.build_chain(vectorstore)\n",
    "        st.write(\"Ready to chat!\")\n",
    "        st.write(\"Ask a question below and I'll try to answer it.\")\n",
    "        question = st.text_input(\"Question:\")\n",
    "        if question:\n",
    "            chat_history = []\n",
    "\n",
    "            response = chain({\"question\": question,  \"chat_history\": chat_history})\n",
    "            chat_history.append((question, response[\"answer\"]))\n",
    "            st.write(response[\"answer\"]) \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
